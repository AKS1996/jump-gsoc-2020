{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: CuArrays.jl found cuda, but did not find libcudnn. Some functionality will not be available.\n",
      "└ @ Flux /home/akshay/.julia/packages/Flux/NpkMm/src/Flux.jl:48\n"
     ]
    }
   ],
   "source": [
    "using JuMP\n",
    "using Flux\n",
    "using Flux: logitcrossentropy, normalise, onecold, onehotbatch, glorot_uniform, @functor\n",
    "using Statistics: mean\n",
    "using Zygote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Nueral Net for classification on Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize hyperparameter arguments\n",
    "struct Args\n",
    "    lr::Float64\n",
    "    repeat::Int\n",
    "    \n",
    "    Args() = new(0.5, 110)\n",
    "end\n",
    "\n",
    "function get_processed_data(args)\n",
    "    labels = Flux.Data.Iris.labels()\n",
    "    features = Flux.Data.Iris.features()\n",
    "\n",
    "    # Subract mean, divide by std dev for normed mean of 0 and std dev of 1.\n",
    "    normed_features = normalise(features, dims=2)\n",
    "\n",
    "    klasses = sort(unique(labels))\n",
    "    onehot_labels = onehotbatch(labels, klasses)\n",
    "\n",
    "    # Split into training and test sets, 2/3 for training, 1/3 for test.\n",
    "    train_indices = [1:3:150 ; 2:3:150]\n",
    "\n",
    "    X_train = normed_features[:, train_indices]\n",
    "    y_train = onehot_labels[:, train_indices]\n",
    "\n",
    "    X_test = normed_features[:, 3:3:150]\n",
    "    y_test = onehot_labels[:, 3:3:150]\n",
    "\n",
    "    #repeat the data `args.repeat` times\n",
    "    train_data = Iterators.repeated((X_train, y_train), args.repeat)\n",
    "    test_data = (X_test,y_test)\n",
    "\n",
    "    return train_data, test_data\n",
    "end\n",
    "\n",
    "# Accuracy Function\n",
    "accuracy(x, y, model) = mean(onecold(model(x)) .== onecold(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training.\n",
      "Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "function train()\n",
    "    args = Args()\n",
    "    \n",
    "    #Loading processed data\n",
    "    train_data, test_data = get_processed_data(args)\n",
    "\n",
    "    model = Chain(Dense(4, 10), Dense(10,3))\n",
    "\n",
    "    \n",
    "    # TODO: Try defining QP layer such as sparsemax here\n",
    "    # minimize ||x − y||^2\n",
    "    #     s.t. 1'y = 1\n",
    "    #          0 ≤ y ≤ 1\n",
    "\n",
    "    # Defining loss function to be used in training\n",
    "    # For numerical stability, we use here logitcrossentropy\n",
    "    loss(x, y) = logitcrossentropy(model(x), y)\n",
    "\n",
    "    # Training\n",
    "    # Gradient descent optimiser with learning rate `args.lr`\n",
    "    optimiser = Descent(args.lr)\n",
    "\n",
    "    println(\"Starting training.\")\n",
    "    Flux.train!(loss, params(model), train_data, optimiser)\n",
    "\n",
    "    return model, test_data\n",
    "end\n",
    "\n",
    "function test(model, test)\n",
    "    # Testing model performance on test data \n",
    "    X_test, y_test = test\n",
    "    accuracy_score = accuracy(X_test, y_test, model)\n",
    "\n",
    "    println(\"Accuracy: $accuracy_score\")\n",
    "end\n",
    "\n",
    "model, test_data = train()\n",
    "test(model, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a QP layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct QPLayer\n",
    "    Q::Array{Float64}\n",
    "    p::Array{Float64}\n",
    "    A::Array{Float64}\n",
    "    b::Array{Float64}\n",
    "    G::Array{Float64}\n",
    "    h::Array{Float64}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn it into a callable\n",
    "@functor QPLayer\n",
    "\n",
    "function (a::QPLayer)(x::AbstractArray)\n",
    "    W, b = a.W, a.b\n",
    "    (W*x .+ b)\n",
    "    # here should be the forward pass\n",
    "    # TODO: Use qpth logic here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Custom adjoint not defined but not reflected in Flux training\n",
    "Zygote.@adjoint QPLayer(a, b) = QPLayer(a, b), qp -> (qp.b, qp.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.5",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
