{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP\n",
    "using Flux\n",
    "using Flux: logitcrossentropy, normalise, onecold, onehotbatch, glorot_uniform, @functor\n",
    "using Statistics: mean\n",
    "using Zygote\n",
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Nueral Net for classification on Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize hyperparameter arguments\n",
    "struct Args\n",
    "    lr::Float64\n",
    "    repeat::Int\n",
    "    \n",
    "    Args() = new(0.5, 110)\n",
    "end\n",
    "\n",
    "function get_processed_data(args)\n",
    "    labels = Flux.Data.Iris.labels()\n",
    "    features = Flux.Data.Iris.features()\n",
    "\n",
    "    # Subract mean, divide by std dev for normed mean of 0 and std dev of 1.\n",
    "    normed_features = normalise(features, dims=2)\n",
    "\n",
    "    klasses = sort(unique(labels))\n",
    "    onehot_labels = onehotbatch(labels, klasses)\n",
    "\n",
    "    # Split into training and test sets, 2/3 for training, 1/3 for test.\n",
    "    train_indices = [1:3:150 ; 2:3:150]\n",
    "\n",
    "    X_train = normed_features[:, train_indices]\n",
    "    y_train = onehot_labels[:, train_indices]\n",
    "\n",
    "    X_test = normed_features[:, 3:3:150]\n",
    "    y_test = onehot_labels[:, 3:3:150]\n",
    "\n",
    "    #repeat the data `args.repeat` times\n",
    "    train_data = Iterators.repeated((X_train, y_train), args.repeat)\n",
    "    test_data = (X_test,y_test)\n",
    "\n",
    "    return train_data, test_data\n",
    "end\n",
    "\n",
    "# Accuracy Function\n",
    "accuracy(x, y, model) = mean(onecold(model(x)) .== onecold(y))\n",
    "\n",
    "# Testing model performance on test data \n",
    "function test(model, test)\n",
    "    X_test, y_test = test\n",
    "    accuracy_score = accuracy(X_test, y_test, model)\n",
    "    println(\"Accuracy: $accuracy_score\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training.\n",
      "Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "function train()\n",
    "    args = Args()\n",
    "    \n",
    "    #Loading processed data\n",
    "    train_data, test_data = get_processed_data(args)\n",
    "\n",
    "    model = Chain(Dense(4,10), Dense(10,3), X -> relu.(X))\n",
    "\n",
    "    # Defining loss function to be used in training\n",
    "    # For numerical stability, we use here logitcrossentropy\n",
    "    loss(x, y) = logitcrossentropy(model(x), y)\n",
    "\n",
    "    # Training\n",
    "    # Gradient descent optimiser with learning rate `args.lr`\n",
    "    optimiser = Descent(args.lr)\n",
    "\n",
    "    println(\"Starting training.\")\n",
    "    Flux.train!(loss, params(model), train_data, optimiser)\n",
    "\n",
    "    return model, test_data\n",
    "end\n",
    "\n",
    "model, test_data = train()\n",
    "test(model, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN using relu defined by QPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train()\n",
    "    args = Args()\n",
    "    \n",
    "    #Loading processed data\n",
    "    train_data, test_data = get_processed_data(args)\n",
    "    \n",
    "    # Using a Relu layer defined by qpth\n",
    "    # y = f(x) = max(0,x) \n",
    "     \n",
    "    # minimize  1/2||x − y||^2\n",
    "    # s.t.      0 ≤ y \n",
    "    \n",
    "    # dont pass matrices, pass a model instead, that way you can expand later\n",
    "\n",
    "    model = Chain(Dense(4,10), Dense(10,3), QPTH(,Matrix(I*-1.0,3,3),zeros(3)))\n",
    "\n",
    "    # Defining loss function to be used in training\n",
    "    # For numerical stability, we use here logitcrossentropy\n",
    "    loss(x, y) = logitcrossentropy(model(x), y)\n",
    "\n",
    "    # Training\n",
    "    # Gradient descent optimiser with learning rate `args.lr`\n",
    "    optimiser = Descent(args.lr)\n",
    "\n",
    "    println(\"Starting training.\")\n",
    "    Flux.train!(loss, params(model), train_data, optimiser)\n",
    "\n",
    "    return model, test_data\n",
    "end\n",
    "\n",
    "model, test_data = train()\n",
    "test(model, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a QP layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct QPTH\n",
    "    Q::Array{Float64}\n",
    "    p::Array{Float64}\n",
    "    G::Array{Float64}\n",
    "    h::Array{Float64}\n",
    "    A::Array{Float64}\n",
    "    b::Array{Float64}\n",
    "    \n",
    "    QPTH(Q_::Array{Float64},p_::Array{Float64},G_::Array{Float64},h_::Array{Float64}) = new(Q_,p_,G_,h_,zeros(0),zeros(0))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn it into a callable\n",
    "@functor QPTH\n",
    "\n",
    "function (qp::QPTH)(x::AbstractArray)\n",
    "    Q_,p_,G_,h_,A_,b_ = qp.Q_,p_,G_,h_\n",
    "    W, b = a.W, a.b\n",
    "    (W*x .+ b)\n",
    "    # here should be the forward pass\n",
    "    # TODO: Use qpth logic here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Custom adjoint not defined but not reflected in Flux training\n",
    "Zygote.@adjoint QPLayer(a, b) = QPLayer(a, b), qp -> (qp.b, qp.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       " -1.0   0.0   0.0\n",
       "  0.0  -1.0   0.0\n",
       "  0.0   0.0  -1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Matrix(I*-1.0,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.5",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
